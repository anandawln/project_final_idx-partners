# -*- coding: utf-8 -*-
"""Final_Project_ID/X_Partners

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AeW6rO3xHwR3LdxLjIlXNja0vabD7MQs

<h1 style="text-align: center;">Final Task ID/X Partners Data Scientist Project Based Internship</h1>

"This project aims to assist multifinance companies in improving the accuracy of credit assessment and risk management through the development of a machine learning model. This model will predict credit risk based on a dataset that includes data on approved and rejected loans. The goal is to optimize business decisions, reduce potential losses, and increase efficiency in the loan issuance process."

### Loading Data
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

# Load Data
df = pd.read_csv('/content/loan_data_2007_2014.csv')
pd.set_option('display.max_columns', None)
df.head()

"""### Data Understanding"""

# Check dimensions of the dataframe
print("The dimensions of the dataframe are: ", df.shape)

# Check info of the dataframe
print(df.info())

# Check descriptive statistics of the dataframe
df.describe()

# Check descriptive statistics of the categorical columns
df.describe(include='object')

# Check columns with missing values
missing_values = df.isnull().sum()
missing_values = missing_values[missing_values > 0]

# Sort the result in ascending order
missing_values = missing_values.sort_values(ascending=True)
missing_values

# Check duplicate rows
duplicate_rows = df.duplicated().sum()
print("The number of duplicate rows are: ", duplicate_rows)

# Checking unique value (exclude column with no value)
unique_values = df.nunique().sort_values(ascending=True)
unique_values = unique_values[unique_values > 0]
unique_values

"""### Data Preprocessing"""

# Removes columns that do not have values
df = df.dropna(axis=1, how='all')
df.info()

# Drop columns with 1 unique value, redudant, >50% missing value, and unrelevant
dropped = ['Unnamed: 0','id', 'member_id', 'funded_amnt', 'funded_amnt_inv', 'sub_grade', 'emp_title','url', 'desc', 'title', 'zip_code',
           'mths_since_last_delinq', 'mths_since_last_record', 'out_prncp_inv', 'total_pymnt' ,'total_pymnt_inv', 'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee', 'recoveries',
            'collection_recovery_fee', 'last_pymnt_d', 'next_pymnt_d', 'last_credit_pull_d', 'mths_since_last_major_derog', 'policy_code', 'application_type', 'total_rev_hi_lim']

df.drop(columns = dropped, axis=1, inplace=True)

# Check info
df.info()

# Checking for missing values
missing_values = df.isnull().sum()
missing_values[missing_values > 0]

# Checking for unique values column 'loan_status'
print(df['loan_status'].value_counts())

"""Loan status serves as the target variable in this investigation and classified into several categories, specifically:
1. Good: Fully Paid, Current, In Grace Period
2. Bad: Charged Off, Default, Late (31-120 days), Late (16-30 days)


The status "Does not meet the credit policy" is excluded from the analysis because it does not provide final information about the borrower's risk. This means that these loans did not meet the credit policy from the start, and their status of "Fully Paid" or "Charged Off" may not reflect the same conditions as other categories. In other words, these loans have different characteristics from loans approved according to credit policy and may cause bias in the analysis. Therefore, to get a more accurate picture of loan risk, loans with this status are excluded from the analysis.




"""

# Labeling the target variable
# Classify loan_status into binary categories (Good/Bad)
def classify_loan_status(status):
    if status in ['Charged Off', 'Default', 'Late (31-120 days)', 'Late (16-30 days)']:
        return 'Bad'
    elif status in ['Fully Paid', 'Current', 'In Grace Period']:
        return 'Good'
    else:
        return None  # For irrelevant status

# Create a new column
df['loan_risk'] = df['loan_status'].apply(classify_loan_status)

# Drop rows with None (to focus on binary categories)
df = df[df['loan_risk'].notnull()]

# Check the distribution of the target
print(df['loan_risk'].value_counts())

# Mapping 'emp_length' column
emp_length_mapping = {
    '< 1 year': 0.5,
    '1 year': 1,
    '2 years': 2,
    '3 years': 3,
    '4 years': 4,
    '5 years': 5,
    '6 years': 6,
    '7 years': 7,
    '8 years': 8,
    '9 years': 9,
    '10+ years': 10
}
df['emp_length'] = df['emp_length'].map(emp_length_mapping).astype(float)

# Change value'home_ownership'column
df['home_ownership'] = df['home_ownership'].replace(['NONE', 'ANY'], 'OTHER')

# Check data type
df['home_ownership'].value_counts()

# Convert 'term' to integer
df['term'] = df['term'].str.replace(' months', '').astype(int)

# Convert 'issue_d' to datetime with the specified format
df['issue_d'] = pd.to_datetime(df['issue_d'], format='%b-%y')

# Convert 'earliest_cr_line' to datetime first, then correct incorrect years
df['earliest_cr_line'] = pd.to_datetime(df['earliest_cr_line'], format='%b-%y')

# Define function to replace incorrect years
def correct_year(date):
    if date.year > 2011:  # If year > 2011, it's considered incorrect
        # Reduce years greater than 2011 by 100 (e.g., 2044 becomes 1944)
        return date.replace(year=date.year - 100)
    return date

# Apply the function to fix incorrect years in 'earliest_cr_line'
df['earliest_cr_line'] = df['earliest_cr_line'].apply(correct_year)

df.info()

#  Check column with missing values
missing_values = df.isnull().sum()
missing_values = missing_values[missing_values > 0]
missing_values

# Impute missing values in 'emp_length' with the mode
df['emp_length'] = df['emp_length'].fillna(df['emp_length'].mode()[0])

# Impute 'revol_util' with the median
df['revol_util'] = df['revol_util'].fillna(df['revol_util'].median())

# Impute missing values in 'collections_12_mths_ex_med' with 0
df['collections_12_mths_ex_med'] = df['collections_12_mths_ex_med'].fillna(0)

# Drop tot_coll_amt, tot_cur_bal, and loan_status
df = df.drop(columns=['tot_coll_amt', 'tot_cur_bal', 'loan_status'])

# Check remaining missing values
print(df.isnull().sum())

# Separate numerical and categorical columns
numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
datetime_cols = df.select_dtypes(include=['datetime64[ns]']).columns.tolist()

print("Numerical Columns:", numerical_cols)
print("Categorical Columns:", categorical_cols)
print("Datetime Columns:", datetime_cols)

"""### Exploratory Data Analysis (EDA)"""

# Univariate Analysis
# Numerical
print("\nDescriptive Statistics for Numerical Columns:")
print(df[numerical_cols].describe())

plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_cols, 1):
    plt.subplot(5, 4, i)
    sns.histplot(df[col], bins=30, kde=True)
    plt.title(f"Histogram of {col}")
    plt.tight_layout()
plt.show()

""""There are anomalies in the following columns:

1. revol_util: The maximum value of 892.3% is unrealistic. Revolving utilization should range from 0-100% (percentage of revolving credit usage).
2. open_acc: The minimum value of 0 is unusual, as someone with a loan should have at least 1 open account. This could be due to a new account being opened but not yet recorded, or an error. This can be handled by imputing a value of 1.
3. annual_inc and revol_bal: Extreme outliers can be handled by capping
"""

# Categorical
for col in categorical_cols:
    print(f"\nValue Counts for {col}:")
    print(df[col].value_counts())

plt.figure(figsize=(15, 10))
for i, col in enumerate(categorical_cols, 1):
    plt.subplot(3, 3, i)
    sns.countplot(x=col, data=df)
    plt.title(f"Count Plot of {col}")
    plt.xticks(rotation=45)
    plt.tight_layout()
plt.show()

# Datetime
df['issue_d_year'] = df['issue_d'].dt.year
df['earliest_cr_line_year'] = df['earliest_cr_line'].dt.year
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
sns.histplot(df['issue_d_year'], bins=20)
plt.title("Distribution of issue_d Year")
plt.subplot(1, 2, 2)
sns.histplot(df['earliest_cr_line_year'], bins=20)
plt.title("Distribution of earliest_cr_line Year")
plt.tight_layout()
plt.show()

# Step 3: Bivariate Analysis
# Numerical vs loan_risk
plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_cols, 1):
    plt.subplot(5, 4, i)
    sns.boxplot(x='loan_risk', y=col, data=df)
    plt.title(f"{col} vs loan_risk")
    plt.tight_layout()
plt.show()

# Categorical vs loan_risk
for col in categorical_cols:
    if col != 'loan_risk':
        crosstab = pd.crosstab(df[col], df['loan_risk'], normalize='index')
        print(f"\nCrosstab {col} vs loan_risk:")
        print(crosstab)
        plt.figure(figsize=(8, 6))
        sns.heatmap(crosstab, annot=True, cmap="YlGnBu", fmt='.2f')
        plt.title(f"{col} vs loan_risk")
        plt.show()

# Datetime vs loan_risk
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
sns.boxplot(x='loan_risk', y='issue_d_year', data=df)
plt.title("issue_d Year vs loan_risk")
plt.subplot(1, 2, 2)
sns.boxplot(x='loan_risk', y='earliest_cr_line_year', data=df)
plt.title("earliest_cr_line Year vs loan_risk")
plt.tight_layout()
plt.show()

# Step 4: Multivariate Analysis
# Correlation matrix
plt.figure(figsize=(12, 8))
correlation_matrix = df[numerical_cols].corr()
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt='.2f', linewidths=0.5)
plt.title("Correlation Matrix of Numerical Columns")
plt.show()

"""### Data Preparation"""

# Find column with corelation > 0.7
correlation_matrix = df[numerical_cols].corr().abs()
correlation_matrix = correlation_matrix.unstack().sort_values(ascending=False).reset_index()
correlation_matrix.columns = ['Feature 1', 'Feature 2', 'Correlation']
correlation_matrix = correlation_matrix[correlation_matrix['Feature 1'] != correlation_matrix['Feature 2']]
correlation_matrix = correlation_matrix[correlation_matrix['Correlation'] > 0.7]
correlation_matrix

"""Dropped the installment variable to avoid multicollinearity, as it is derived from loan_amnt"""

# Drop installment (assuming it's the one to drop)
df = df.drop(columns=['installment'], axis=1)
print("Columns after dropping:", df.columns)

# Drop column issue_d and earliest_cr_line
df = df.drop(columns=['issue_d', 'earliest_cr_line'], axis=1)
df.info()

# Handling Outliers
# Cap outliers in 'annual_inc', 'revol_bal', 'open_acc' and 'revol_util'

df['revol_util'] = df['revol_util'].apply(lambda x: min(x, 100))
def cap_outliers_iqr(df, column):
    q1 = df[column].quantile(0.25)
    q3 = df[column].quantile(0.75)
    iqr = q3 - q1
    upper_bound = q3 + 1.5 * iqr
    df[column] = df[column].apply(lambda x: min(x, upper_bound))
    return df

df = cap_outliers_iqr(df, 'annual_inc')
df = cap_outliers_iqr(df, 'revol_bal')
df.loc[df['open_acc'] == 0, 'open_acc'] = 1

# Encoding
from sklearn.preprocessing import LabelEncoder

# Encode target
label_encoder = LabelEncoder()
df['loan_risk_encoded'] = label_encoder.fit_transform(df['loan_risk'])

# One-hot encoding for categorical features
categorical_cols = ['grade', 'home_ownership', 'verification_status', 'pymnt_plan',
                    'purpose', 'addr_state', 'initial_list_status']
df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

# Train-Test Split
from sklearn.model_selection import train_test_split

X = df_encoded.drop(columns=['loan_risk', 'loan_risk_encoded'])
y = df_encoded['loan_risk_encoded']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Scaling
from sklearn.preprocessing import StandardScaler

# Pilih kolom numerik (termasuk dari encoding)
numerical_cols = X_train.select_dtypes(include=['int64', 'float64', 'int32', 'uint8']).columns.tolist()

# Terapkan scaling
scaler = StandardScaler()
X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])
X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])

# SMOTE
from imblearn.over_sampling import SMOTE

# Cek distribusi kelas sebelum SMOTE
print("Before SMOTE:")
print(y_train.value_counts())

# Terapkan SMOTE
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Cek distribusi kelas setelah SMOTE
print("After SMOTE:")
print(y_train_smote.value_counts())

"""### Data Modelling"""

import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import label_binarize

# Asumsi: X_train_smote, y_train_smote, X_test, y_test sudah ada
n_classes = len(np.unique(y_train_smote))
print("Number of classes:", n_classes)

# Binarize y_test untuk multi-kelas
y_test_bin = label_binarize(y_test, classes=np.arange(n_classes))

# 1. Logistic Regression
print("=== Logistic Regression ===")
lr_model = LogisticRegression(max_iter=1000, random_state=42, multi_class='multinomial')
lr_model.fit(X_train_smote, y_train_smote)
y_pred_lr = lr_model.predict(X_test)
y_score_lr = lr_model.predict_proba(X_test)

# Cek bentuk
print("Shape of y_test_bin:", y_test_bin.shape)
print("Shape of y_score_lr:", y_score_lr.shape)

# Evaluasi
print("Classification Report:")
print(classification_report(y_test, y_pred_lr))
cm_lr = confusion_matrix(y_test, y_pred_lr)
plt.figure(figsize=(6, 4))
sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Logistic Regression")
plt.show()

# ROC-AUC
if n_classes == 2:  # Jika biner
    roc_auc_lr = roc_auc_score(y_test, y_score_lr[:, 1])
    print(f"ROC-AUC Score (Binary): {roc_auc_lr:.4f}")
else:  # Jika multi-kelas
    roc_auc_lr = roc_auc_score(y_test_bin, y_score_lr, multi_class='ovr')
    print(f"ROC-AUC Score (OvR): {roc_auc_lr:.4f}")

import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, roc_curve
from sklearn.preprocessing import label_binarize
import matplotlib.pyplot as plt
import seaborn as sns

# Asumsi: X_train_smote, y_train_smote, X_test, y_test sudah ada
n_classes = len(np.unique(y_train_smote))
print("Number of classes:", n_classes)  # Debug: pastikan integer
print("Type of n_classes:", type(n_classes))  # Debug: pastikan tipe integer

# Binarize y_test untuk multi-kelas
y_test_bin = label_binarize(y_test, classes=np.arange(n_classes))

# Fungsi untuk Plot ROC Curve (konsisten untuk biner dan multi-kelas)
def plot_roc_curve(y_true, y_score, n_classes, model_name="Random Forest"):
    plt.figure(figsize=(8, 6))

    if n_classes == 2:  # Klasifikasi Biner
        fpr, tpr, _ = roc_curve(y_true, y_score[:, 1])  # Probabilitas kelas positif
        roc_auc = roc_auc_score(y_true, y_score[:, 1])
        plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.4f})')
    else:  # Multi-Kelas (One-vs-Rest)
        for i in range(n_classes):
            fpr, tpr, _ = roc_curve(y_true[:, i], y_score[:, i])
            roc_auc = roc_auc_score(y_true[:, i], y_score[:, i])
            plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc:.4f})')

    plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve - {model_name}')
    plt.legend(loc="lower right")
    plt.show()

# Random Forest
print("\n=== Random Forest ===")
rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
rf_model.fit(X_train_smote, y_train_smote)
y_pred_rf = rf_model.predict(X_test)
y_score_rf = rf_model.predict_proba(X_test)
print("Shape of y_test:", y_test.shape)
print("Shape of y_score_rf:", y_score_rf.shape)
print("Classification Report:")
print(classification_report(y_test, y_pred_rf))
cm_rf = confusion_matrix(y_test, y_pred_rf)
plt.figure(figsize=(6, 4))
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Random Forest")
plt.show()

# ROC-AUC dan Plot
if n_classes == 2:  # Biner
    roc_auc_rf = roc_auc_score(y_test, y_score_rf[:, 1])
    print(f"ROC-AUC Score (Binary): {roc_auc_rf:.4f}")
    plot_roc_curve(y_test, y_score_rf, n_classes, "Random Forest")
else:  # Multi-Kelas
    roc_auc_rf = roc_auc_score(y_test_bin, y_score_rf, multi_class='ovr')
    print(f"ROC-AUC Score (OvR): {roc_auc_rf:.4f}")
    plot_roc_curve(y_test_bin, y_score_rf, n_classes, "Random Forest")

import numpy as np
from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, roc_curve
from sklearn.preprocessing import label_binarize
import matplotlib.pyplot as plt
import seaborn as sns

# Asumsi: X_train_smote, y_train_smote, X_test, y_test sudah ada
n_classes = len(np.unique(y_train_smote))
print("Number of classes:", n_classes)  # Debug: pastikan nilai
print("Type of n_classes:", type(n_classes))  # Debug: pastikan tipe integer

# Binarize y_test untuk multi-kelas
y_test_bin = label_binarize(y_test, classes=np.arange(n_classes))

# Fungsi untuk Plot ROC Curve (konsisten untuk biner dan multi-kelas)
def plot_roc_curve(y_true, y_score, n_classes, model_name="XGBoost"):
    plt.figure(figsize=(8, 6))

    if n_classes == 2:  # Klasifikasi Biner
        fpr, tpr, _ = roc_curve(y_true, y_score[:, 1])  # Probabilitas kelas positif
        roc_auc = roc_auc_score(y_true, y_score[:, 1])
        plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.4f})')
    else:  # Multi-Kelas (One-vs-Rest)
        if not isinstance(n_classes, int):  # Cek tipe n_classes
            raise ValueError(f"n_classes harus integer, bukan {type(n_classes)}")
        for i in range(n_classes):
            fpr, tpr, _ = roc_curve(y_true[:, i], y_score[:, i])
            roc_auc = roc_auc_score(y_true[:, i], y_score[:, i])
            plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc:.4f})')

    plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve - {model_name}')
    plt.legend(loc="lower right")
    plt.show()

# XGBoost
print("\n=== XGBoost ===")
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42, n_jobs=-1)
xgb_model.fit(X_train_smote, y_train_smote)
y_pred_xgb = xgb_model.predict(X_test)
y_score_xgb = xgb_model.predict_proba(X_test)
print("Shape of y_test:", y_test.shape)
print("Shape of y_test_bin:", y_test_bin.shape)
print("Shape of y_score_xgb:", y_score_xgb.shape)
print("Classification Report:")
print(classification_report(y_test, y_pred_xgb))
cm_xgb = confusion_matrix(y_test, y_pred_xgb)
plt.figure(figsize=(6, 4))
sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - XGBoost")
plt.show()

# ROC-AUC dan Plot
if n_classes == 2:  # Biner
    roc_auc_xgb = roc_auc_score(y_test, y_score_xgb[:, 1])
    print(f"ROC-AUC Score (Binary): {roc_auc_xgb:.4f}")
    plot_roc_curve(y_test, y_score_xgb, n_classes, "XGBoost")
else:  # Multi-Kelas
    roc_auc_xgb = roc_auc_score(y_test_bin, y_score_xgb, multi_class='ovr')
    print(f"ROC-AUC Score (OvR): {roc_auc_xgb:.4f}")
    plot_roc_curve(y_test_bin, y_score_xgb, n_classes, "XGBoost")

"""### Evaluation"""

# Final Model Selection and Evaluation (based on ROC-AUC)
models = {
    'Logistic Regression': (roc_auc_lr, lr_model),
    'Random Forest': (roc_auc_rf, rf_model),
    'XGBoost': (roc_auc_xgb, xgb_model)
}

best_model_name = max(models, key=lambda k: models[k][0])
best_model_auc = models[best_model_name][0]
best_model = models[best_model_name][1]

print(f"\nThe best model is {best_model_name} with ROC-AUC: {best_model_auc:.4f}")

# Feature Importance (if applicable)
if hasattr(best_model, 'feature_importances_'):
    feature_importances = pd.Series(best_model.feature_importances_, index=X_train.columns)
    feature_importances = feature_importances.sort_values(ascending=False)
    print("\nFeature Importances:")
    print(feature_importances)
    plt.figure(figsize=(10,6))
    sns.barplot(x=feature_importances.values, y=feature_importances.index)
    plt.title('Feature Importances')
    plt.show()

